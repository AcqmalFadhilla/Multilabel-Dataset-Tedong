{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilabel with Segmentation U-Net, FCN & Mask RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import keras as K\n",
    "import matplotlib.cm as cm\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.losses import binary_crossentropy\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import TensorBoard\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/label.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buat daftar unique dari semua label\n",
    "unique_label = set()\n",
    "for label in df[\"Label\"]:\n",
    "    label_list = label.strip(\"[]\").split(\",\")\n",
    "    for label in label_list:\n",
    "        unique_label.add(label.strip())\n",
    "        \n",
    "# buatkan kolom one-hot encodinf setiap label\n",
    "for label in unique_label:\n",
    "    df[label] = df[\"Label\"].apply(lambda x: int(label in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Filename\"] = \"data/image/\" + df[\"Filename\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = np.random.randint(0, len(df), 10)\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 10),\n",
    "                    subplot_kw={'xticks': [], 'yticks': []})\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(plt.imread(df.Filename[random_index[i]]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Filename\"] = 'data/image/' + df[\"Filename\"]\n",
    "df.drop(columns='Label', axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Truk\"\t,\"Kerbau\",\t\"Motor\",\t\"Hewan liar\",\t\"Manusia\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "# The value for class_mode in flow_from_dataframe MUST be 'raw' if you are attempting to do multilabel classification.\n",
    "train_gen = train_datagen.flow_from_dataframe(train, \n",
    "                                              x_col='Filename', \n",
    "                                              y_col=columns,\n",
    "                                              target_size=(224,224),\n",
    "                                              class_mode='raw',\n",
    "                                              batch_size=32,\n",
    "                                              shuffle=True,\n",
    "                                              subset='training')\n",
    "val_gen = train_datagen.flow_from_dataframe(train,\n",
    "                                          x_col='Filename',\n",
    "                                          y_col=columns,\n",
    "                                          target_size=(224,224),\n",
    "                                          class_mode='raw',\n",
    "                                          batch_size=16,\n",
    "                                          subset='validation')\n",
    "test_gen = test_datagen.flow_from_dataframe(test,\n",
    "                                            x_col='Filename',\n",
    "                                            y_col=columns,\n",
    "                                            target_size=(224,224),\n",
    "                                            class_mode='raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berikut arsitektur keseluruhan U-Net yang akan digunakan:\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=view&id=1BeQSKL2Eq6Fw9iRXsN1hgunY-CS2nH7V' alt='unet'>\n",
    "\n",
    "UNet terdiri dari encoder (downsampler) dan decoder (upsampler) dengan hambatan di antaranya. Panah abu-abu sesuai dengan koneksi lompatan yang menggabungkan output blok encoder ke setiap tahap decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "Block-block encoder berisi 2 layer Conv2D diakttifkan oleh relu, serta di ikuti oleh MaxpPooling dan Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_block(input_tensor, n_filter, kernel_size=3):\n",
    "    ''' \n",
    "    Arg:\n",
    "    input_tensor (tensor) - tensor masukan\n",
    "    n_filter (int) - jumlah filter\n",
    "    kernel_size (int) -- ukuran kernel untuk konvolusi\n",
    "\n",
    "    return\n",
    "    tensor dari fitur keluaran\n",
    "    '''\n",
    "    x = input_tensor\n",
    "    for i in range(2):\n",
    "        x = tf.keras.layers.Conv2D(filters=n_filter, kernel_size=(kernel_size, kernel_size),\\\n",
    "                                    kernel_initializer=\"he_normal\",padding='same')(x)\n",
    "        x = tf.keras.layers.Activation(\"relu\")(x)\n",
    "    return x\n",
    "\n",
    "def encoder_block(inputs, n_filter=64, pool_size=2, dropout=0.3):\n",
    "    '''\n",
    "    Arg:\n",
    "    input (tensor) - tensor masukan\n",
    "    n_filter (int) - jumlah filter\n",
    "    pool_size - ukuran pool size\n",
    "    dropout - jumlah yang ingin di dropout/buang\n",
    "\n",
    "    return\n",
    "    f - fitur keluaran dari block conv\n",
    "    p - fitur yang dikumpulkan di MaxPooling dengan DropOut\n",
    "    '''\n",
    "    f = conv2d_block(inputs, n_filter=n_filter)\n",
    "    p = tf.keras.layers.MaxPooling2D(pool_size=(pool_size, pool_size))(f)\n",
    "    p = tf.keras.layers.Dropout(dropout)(p)\n",
    "    return f, p\n",
    "\n",
    "def encoder(inputs):\n",
    "    '''\n",
    "    Arg:\n",
    "    inputs - masukkan gambar\n",
    "\n",
    "    return\n",
    "    p4 - fitur yang dikumpulkan dari keseluruhan encoder block\n",
    "    (f1, f2, f3, f4) - fitur yang dikeluarkan dari semua encoder block\n",
    "    '''\n",
    "    f1, p1 = encoder_block(inputs, n_filter=64, pool_size=2, dropout=0.3)\n",
    "    f2, p2 = encoder_block(p1, n_filter=128, pool_size=2, dropout=0.3)\n",
    "    f3, p3 = encoder_block(p2, n_filter=256, pool_size=2, dropout=0.3)\n",
    "    f4, p4 = encoder_block(p3, n_filter=512, pool_size=2, dropout=0.3)\n",
    "    return p4, (f1, f2, f3, f4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bottleneck\n",
    "Sebuah bottleneck mengikuti blok encoder dan digunakan untuk mengekstrak lebih banyak fitur. Ini tidak memiliki lapisan penyatuan sehingga dimensinya tetap sama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bottleneck(inputs):\n",
    "\n",
    "    x = conv2d_block(inputs, n_filter=1024)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    " decoder yang meng-upsampling fitur kembali ke ukuran gambar asli. Pada setiap tingkat upsampling, akan mengambil output dari blok encoder yang sesuai dan menggabungkannya sebelum memasukkannya ke blok decoder berikutnya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block(inputs, conv_output, n_filter=64, kernel_size=3, strides=2, dropout=0.3):\n",
    "    '''\n",
    "    Arg:\n",
    "    inputs (tensor) - kumpulan fitur input\n",
    "    conv_output (tensor) - fitur dari blok encoder\n",
    "    n_filter (int) -- jumlah filter\n",
    "    kernel_size (int) -- ukuran kernel\n",
    "    strides (int) -- langkah untuk dekonvolusi/upsampling\n",
    "    padding (string) - \"sama\" atau \"valid\", memberi tahu apakah bentuk akan dipertahankan dengan padding nol\n",
    "\n",
    "    return\n",
    "    c (tensor) - fitur keluaran dari blok decoder\n",
    "    '''\n",
    "    u = tf.keras.layers.Conv2DTranspose(n_filter, kernel_size, strides=strides, padding='same')(inputs)\n",
    "    c = tf.keras.layers.concatenate([u, conv_output])\n",
    "    c = tf.keras.layers.Dropout(dropout)(c)\n",
    "    c = conv2d_block(c, n_filter, kernel_size=3)\n",
    "\n",
    "    return c\n",
    "\n",
    "def decoder(input, convs, output_channels):\n",
    "    '''\n",
    "    Arg:\n",
    "    inputs (tensor) - kumpulan fitur input\n",
    "    convs (tuple) -- fitur dari blok penyandi\n",
    "    output_channels (int) -- jumlah kelas dalam peta label\n",
    "    activation (softmax/sigmoid,etc) - aktifasi yang dipakai\n",
    "\n",
    "    return\n",
    "    output (tensor) -- peta label berdasarkan piksel dari gambar\n",
    "    '''\n",
    "    f1, f2, f3, f4 = convs\n",
    "\n",
    "    c6 = decoder_block(input, f4, n_filter=512, kernel_size=(3,3), strides=(2,2), dropout=0.3)\n",
    "    c7 = decoder_block(c6, f3, n_filter=256, kernel_size=(3,3), strides=(2,2), dropout=0.3)\n",
    "    c8 = decoder_block(c7, f2, n_filter=128, kernel_size=(3,3), strides=(2,2), dropout=0.3)\n",
    "    c9 = decoder_block(c8, f1, n_filter=64, kernel_size=(3,3), strides=(2,2), dropout=0.3)\n",
    "\n",
    "    flatten = tf.keras.layers.Flatten()(c9)\n",
    "    output =  tf.keras.layers.Dense(output_channels, activation='sigmoid')(flatten)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet(input_shape):\n",
    "    '''\n",
    "    Mendefinisikan UNet dengan menghubungkan encoder, bottleneck, dan decoder.\n",
    "\n",
    "    args:\n",
    "    input_shape (int)- besaran deminsi input\n",
    "    output (int) - jumlah output yang dikeluarkna\n",
    "    activation - aktifasi yang digunakan untuk klasifikasi\n",
    "    '''\n",
    "\n",
    "    input = tf.keras.layers.Input(shape=input_shape)\n",
    "    encoder_output, convs = encoder(input)\n",
    "    bottlenecks = bottleneck(encoder_output)\n",
    "    outputs = decoder(bottlenecks, convs, output_channels=5)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = tf.keras.metrics.AUC(multi_label=True,thresholds=[0,0.5])\n",
    "aucpr = tf.keras.metrics.AUC(curve='PR',multi_label=True,thresholds=[0,0.5])\n",
    "model_uNet = unet(input_shape=(224, 224, 3,))\n",
    "model_uNet.compile(optimizer='adam', loss=\"binary_crossentropy\",\n",
    "                   metrics=[\"accuracy\", auc, aucpr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uNet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def tensorboard_callback(name):\n",
    "    logdir = os.path.join(\"Tensorboard/logs\", datetime.datetime.now().strftime(f\"%Y-%m-%d-{name}\"))\n",
    "    return TensorBoard(logdir)\n",
    "\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, accuracy=0.9):\n",
    "        self.accuracy = accuracy\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs.get(\"val_accuracy\") > self.accuracy:\n",
    "            print(f\"\\nAkurasi telah mencapai {self.accuracy}%\")\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/device:GPU:0\"):\n",
    "    history1 = model_uNet.fit(train_gen,\n",
    "                              steps_per_epoch=train_gen.n // train_gen.batch_size,\n",
    "                              epochs=50,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_gen.n // val_gen.batch_size,\n",
    "                              callbacks=[tensorboard_callback(\"uNet\"),\n",
    "                              myCallback(accuracy=0.9)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uNet.evaluate(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_common():\n",
    "    inputs = tf.keras.layers.Input(shape=(224, 224, 3))\n",
    "    x = conv2d_block(input_tensor=inputs, n_filter=64)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = conv2d_block(input_tensor=x, n_filter=128)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = conv2d_block(input_tensor=x, n_filter=128)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = conv2d_block(input_tensor=x, n_filter=256)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = conv2d_block(input_tensor=x, n_filter=512)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    output = tf.keras.layers.Dense(5, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_common.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_common = model_common()\n",
    "model_common.compile(optimizer='adam', loss=\"binary_crossentropy\",\n",
    "                   metrics=[\"accuracy\", auc, aucpr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model_common.fit(train_gen,\n",
    "                              steps_per_epoch=train_gen.n // train_gen.batch_size,\n",
    "                              epochs=50,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_gen.n // val_gen.batch_size,\n",
    "                              callbacks=[tensorboard_callback(\"common\"),\n",
    "                              myCallback(accuracy=0.9)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_common.evaluate(test_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Vis Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_name(model):\n",
    "    outputs = [layer.outputs for layer in model.layer]\n",
    "    layer_name = []\n",
    "    for layer in outputs:\n",
    "        layer_name.append(layer.name.split(\"/\"))\n",
    "    \n",
    "    return layer_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_intermediate_activations(layer_names, activations):\n",
    "    assert len(layer_names)==len(activations), \"Make sure layers and activation values match\"\n",
    "    images_per_row=16\n",
    "    \n",
    "    for layer_name, layer_activation in zip(layer_names, activations):\n",
    "        nb_features = layer_activation.shape[-1]\n",
    "        size= layer_activation.shape[1]\n",
    "\n",
    "        nb_cols = nb_features // images_per_row\n",
    "        grid = np.zeros((size*nb_cols, size*images_per_row))\n",
    "\n",
    "        for col in range(nb_cols):\n",
    "            for row in range(images_per_row):\n",
    "                feature_map = layer_activation[0,:,:,col*images_per_row + row]\n",
    "                feature_map -= feature_map.mean()\n",
    "                feature_map /= feature_map.std()\n",
    "                feature_map *=255\n",
    "                feature_map = np.clip(feature_map, 0, 255).astype(np.uint8)\n",
    "\n",
    "                grid[col*size:(col+1)*size, row*size:(row+1)*size] = feature_map\n",
    "\n",
    "        scale = 1./size\n",
    "        plt.figure(figsize=(scale*grid.shape[1], scale*grid.shape[0]))\n",
    "        plt.title(layer_name)\n",
    "        plt.grid(False)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(grid, aspect='auto', cmap='viridis')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Grad Cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_array(dir, size):\n",
    "    img = keras.utils.load_img(dir, target_size=size)\n",
    "    array = keras.utils.img_to_array(img)\n",
    "    array = np.expand_dims(array)\n",
    "    return array\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    grad_model = keras.models.Model(\n",
    "        model.inputs, [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_display_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n",
    "    # Load the original image\n",
    "    img = keras.utils.load_img(img_path)\n",
    "    img = keras.utils.img_to_array(img)\n",
    "\n",
    "    # Rescale heatmap to a range 0-255\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "    # Use jet colormap to colorize heatmap\n",
    "    jet = cm.get_cmap(\"jet\")\n",
    "\n",
    "    # Use RGB values of the colormap\n",
    "    jet_colors = jet(np.arange(256))[:, :3]\n",
    "    jet_heatmap = jet_colors[heatmap]\n",
    "\n",
    "    # Create an image with RGB colorized heatmap\n",
    "    jet_heatmap = keras.utils.array_to_img(jet_heatmap)\n",
    "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
    "    jet_heatmap = keras.utils.img_to_array(jet_heatmap)\n",
    "\n",
    "    # Superimpose the heatmap on original image\n",
    "    superimposed_img = jet_heatmap * alpha + img\n",
    "    superimposed_img = keras.utils.array_to_img(superimposed_img)\n",
    "\n",
    "    # Save the superimposed image\n",
    "    superimposed_img.save(cam_path)\n",
    "\n",
    "    # Display Grad CAM\n",
    "    display(Image(cam_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
